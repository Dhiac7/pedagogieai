# G√©n√©rateur P√©dagogique IA

Application compl√®te pour g√©rer des comp√©tences p√©dagogiques, cr√©er des questions et g√©n√©rer du contenu √©ducatif avec l'intelligence artificielle. Le syst√®me utilise des mod√®les de langage locaux pour g√©n√©rer des questions personnalis√©es selon les niveaux scolaires, mati√®res et comp√©tences sp√©cifiques.

### üöÄ Fonctionnalit√©s

- **Gestion P√©dagogique**: niveaux scolaires, mati√®res, th√©matiques, comp√©tences et sous-comp√©tences
- **Questions**: cr√©ation, filtrage, statuts (brouillon, g√©n√©r√©e, valid√©e, archiv√©e)
- **G√©n√©ration IA Locale**: utilisation de mod√®les GGUF (Llama 2) via ctransformers
- **Quiz**: cr√©ation de quiz, association questions, tentatives et r√©ponses
- **Interface Moderne**: Next.js 15 + React 19 + Tailwind CSS v4 + Radix UI
- **API REST**: Django REST Framework avec endpoints complets

### üèóÔ∏è Architecture

- **Backend Principal**: Django 5 + Django REST Framework (port 8000)
- **Service IA**: FastAPI + LangChain (port 8001)
- **T√¢ches**: Celery (ex√©cution synchrone en dev, pas de broker requis)
- **Base de donn√©es**: PostgreSQL (config par d√©faut dans `myproject/settings.py`)
- **Frontend**: Next.js 15 (TypeScript), UI Radix + Tailwind CSS v4
- **IA Locale**: Mod√®les GGUF via ctransformers (Llama 2 7B Chat)
- **LangChain**: Int√©gration pour la gestion des prompts et mod√®les

### ‚úÖ Pr√©requis

- Python 3.10+
- Node.js 18+
- PostgreSQL 14+
- Mod√®le GGUF Llama 2 (inclus: `llama-2-7b-chat.Q4_K_M.gguf`)
- 8GB+ RAM recommand√© pour le mod√®le local

### ‚öôÔ∏è Configuration et installation

#### 1) Backend (Django)

```powershell
# Depuis la racine du projet
pip install -r requirements.txt

# Configurez PostgreSQL selon vos besoins puis migrez
python manage.py migrate

# (Optionnel) donn√©es et prompts par d√©faut
python manage.py load_default_data
python manage.py load_default_prompt

# Lancer l'API Django (port 8000)
python manage.py runserver
```

#### 2) Service IA (FastAPI)

```powershell
# Lancer le service de g√©n√©ration de questions (port 8001)
python questionsGenerator.py
```

**Test du service FastAPI:**
```powershell
# V√©rifier que le service fonctionne
python test_fastapi.py
```

Configuration BD par d√©faut (voir `myproject/settings.py`):

```
ENGINE: django.db.backends.postgresql
NAME:   pedagogieai
USER:   postgres
PASSWORD: ******
HOST:   localhost
PORT:   5432
```

Adaptez ces valeurs directement dans `myproject/settings.py` si n√©cessaire. Un fichier `db.sqlite3` est pr√©sent mais non utilis√© par d√©faut.

Celery est configur√© en mode ¬´ eager ¬ª pour le d√©veloppement (aucun Redis/Broker requis).

Le mod√®le GGUF Llama 2 est inclus dans le dossier `models/`. Le syst√®me utilise ctransformers pour charger le mod√®le localement sans serveur externe.

#### 3) Frontend (Next.js)

```powershell
cd frontend
npm install

# URL de l‚ÄôAPI (PowerShell)
$env:NEXT_PUBLIC_API_URL = "http://localhost:8000"

npm run dev
```

Le frontend est accessible sur `http://localhost:3000` et appelle l‚ÄôAPI Django via `NEXT_PUBLIC_API_URL`.

### üìÅ Structure du projet

```
pedagogieai/
‚îú‚îÄ‚îÄ myapp/                    # App Django (mod√®les, vues, s√©rialiseurs, t√¢ches)
‚îÇ   ‚îú‚îÄ‚îÄ models.py            # Mod√®les de donn√©es (Matiere, Niveau, Competence, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Services IA (ctransformers_client.py, llm_client.py)
‚îÇ   ‚îú‚îÄ‚îÄ management/commands/ # Commandes Django personnalis√©es
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ frontend/                # App Next.js (App Router)
‚îÇ   ‚îú‚îÄ‚îÄ app/                 # Pages et layouts Next.js
‚îÇ   ‚îú‚îÄ‚îÄ components/          # Composants React r√©utilisables
‚îÇ   ‚îú‚îÄ‚îÄ lib/                 # Utilitaires et API client
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ models/                  # Mod√®les GGUF (Llama 2)
‚îÇ   ‚îî‚îÄ‚îÄ llama-2-7b-chat.Q4_K_M.gguf
‚îú‚îÄ‚îÄ myproject/               # Configuration Django
‚îú‚îÄ‚îÄ pedagogieQuestions_logic.py  # Logique de g√©n√©ration de questions
‚îú‚îÄ‚îÄ questionsGenerator.py    # G√©n√©rateur de questions avec LangChain
‚îî‚îÄ‚îÄ requirements.txt         # D√©pendances Python
```

### üîå APIs disponibles

#### API Django (Port 8000)
Base: `http://localhost:8000/api/`

- `GET niveaux/`
- `GET matieres/`
- `GET thematiques/`
- `GET competences/`
- `GET sous-competences/`
- `GET questions/`
- `GET generations/`
- `GET quiz/`
- `POST generate/`

#### API FastAPI IA (Port 8001)
Base: `http://localhost:8001/`

- `GET /` - V√©rification de l'√©tat du service
- `POST /generate-questions` - G√©n√©ration directe de questions p√©dagogiques

#### Exemples d'utilisation

**API Django (g√©n√©ration via Celery):**
```bash
curl -X POST http://localhost:8000/api/generate/ \
  -H "Content-Type: application/json" \
  -d '{
    "competence_id": 1,
    "sous_competence_id": 2,
    "prompt": "G√©n√®re une question de math√©matiques niveau CE2 sur les fractions.",
    "model": "local:llama-2-7b-chat",
    "params": {"temperature": 0.7, "max_new_tokens": 300}
  }'
```

**API FastAPI (g√©n√©ration directe):**
```bash
curl -X POST http://localhost:8001/generate-questions \
  -H "Content-Type: application/json" \
  -d '{
    "school_level": "Grade 5",
    "module": "Mathematics",
    "thematic": "Fractions",
    "competence": "Problem solving",
    "sous_competence": "Add and subtract fractions",
    "num_questions": 3,
    "question_type": "quiz"
  }'
```

Le traitement Django est asynchrone via Celery (ex√©cut√© imm√©diatement en dev). L'API FastAPI retourne directement les questions g√©n√©r√©es.

### ü§ñ G√©n√©ration de Questions IA

Le syst√®me utilise un mod√®le Llama 2 7B Chat quantifi√© (Q4_K_M) pour g√©n√©rer des questions p√©dagogiques personnalis√©es. Les prompts sont optimis√©s pour:

- **Niveaux scolaires**: Adaptation du contenu selon l'√¢ge et le niveau
- **Comp√©tences sp√©cifiques**: Focus sur les sous-comp√©tences √† √©valuer
- **Types de questions**: Quiz, QCM, questions ouvertes, etc.
- **Contexte p√©dagogique**: Alignement avec les programmes √©ducatifs

### üß™ Donn√©es de d√©mo

- `python manage.py load_default_data` cr√©e des niveaux, mati√®res, th√©matiques, comp√©tences et quelques templates de prompt.
- `python manage.py load_default_prompt` ajoute un template g√©n√©rique `template_par_defaut`.

### üîê S√©curit√© et CORS

- `DEBUG=True` en dev et permissions ouvertes (toutes les routes accessibles). √Ä verrouiller en production.
- CORS autorise `http://localhost:3000` par d√©faut.

### üß∞ D√©veloppement quotidien

- Terminal 1 (API Django): `python manage.py runserver` (port 8000)
- Terminal 2 (Service IA): `python questionsGenerator.py` (port 8001)
- Terminal 3 (Frontend): `cd frontend && npm run dev` (port 3000)
- Le mod√®le IA se charge automatiquement au premier appel

### ‚ùóD√©pannage

- **Connexion PostgreSQL**: v√©rifiez les identifiants dans `myproject/settings.py` et que le serveur tourne sur le port 5432.
- **CORS**: si vous changez l'URL du frontend, ajoutez-la dans `CORS_ALLOWED_ORIGINS` et `CSRF_TRUSTED_ORIGINS`.
- **Mod√®le IA**: assurez-vous que le fichier `models/llama-2-7b-chat.Q4_K_M.gguf` est pr√©sent et accessible.
- **M√©moire**: le mod√®le n√©cessite environ 4-6GB de RAM. Ajustez `CTRANSFORMERS_THREADS` et `CTRANSFORMERS_GPU_LAYERS` si n√©cessaire.
- **Ports**: Django (8000), FastAPI (8001), Frontend (3000). Adaptez `NEXT_PUBLIC_API_URL` c√¥t√© frontend si n√©cessaire.
- **Service FastAPI**: v√©rifiez que `python questionsGenerator.py` fonctionne et r√©pond sur le port 8001.
- **CORS**: le service FastAPI est configur√© pour accepter les requ√™tes depuis `localhost:3000`.
- **Test**: utilisez `python test_fastapi.py` pour v√©rifier le bon fonctionnement du service.

### üÜï Nouvelles Fonctionnalit√©s

- **Mod√®le IA Local**: Int√©gration de Llama 2 7B Chat via ctransformers
- **Interface Moderne**: Dashboard avec Next.js 15 et Tailwind CSS v4
- **G√©n√©ration Contextuelle**: Questions adapt√©es aux comp√©tences et niveaux sp√©cifiques
- **API REST Compl√®te**: Endpoints pour toutes les entit√©s p√©dagogiques
- **Gestion des √âtats**: Syst√®me de statuts pour les questions et g√©n√©rations

### üõ†Ô∏è Technologies Utilis√©es

**Backend:**
- Django 5.0 + Django REST Framework (port 8000)
- FastAPI + LangChain (port 8001)
- PostgreSQL + psycopg2
- Celery (mode eager pour le d√©veloppement)
- ctransformers (mod√®les GGUF)
- uvicorn (serveur FastAPI)

**Frontend:**
- Next.js 15 (App Router)
- React 19
- TypeScript
- Tailwind CSS v4
- Radix UI (composants)
- Lucide React (ic√¥nes)

**IA/ML:**
- Llama 2 7B Chat (Q4_K_M quantifi√©)
- ctransformers (inference locale)
- LangChain (orchestration)

### üìÑ Licence

Projet priv√©/interne (ajustez selon vos besoins).